{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Engagment Demo\n",
    "Using a little machine learning, can we tell this difference between and engaged-unengaged task?\n",
    "\n",
    "Description:\n",
    "In this second exercise, we will learn how to use an automatic algorithm to\n",
    "recognize somebody's mental states from their EEG. We will use a classifier,\n",
    "i.e., an algorithm that, provided some data, learns to recognize patterns,\n",
    "and can then classify similar unseen information.\n",
    "\n",
    "## Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Module that simplifies computations on matrices\n",
    "import matplotlib.pyplot as plt  # Module used for plotting\n",
    "from pylsl import StreamInlet, resolve_byprop  # Module to receive EEG data\n",
    "\n",
    "\n",
    "\n",
    "import bci_workshop_tools as BCIw  # Our own functions for the workshop\n",
    "import winsound\n",
    "frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "%matplotlib tk\n",
    "print('Setup Complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection and analysis\n",
    "We will show the computer 30 seconds of \"neutral\" cognitive data, followed by 30 second of \"engaged\" data. The computer will then try and match new data to it's learned data. It should be able to tell a binary difference between the two states.\n",
    "![alt text](rt_ML_figure2.png \"Engagement Decision Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for an EEG stream...\n",
      "Im ready\n",
      "IM ready\n",
      "IM READY\n",
      "\n",
      " Stimuli \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tdgri\\.julia\\conda\\3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Ctrl-C in the console to break the while loop.\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[0.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "Closed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\" 1. CONNECT TO EEG STREAM \"\"\"\n",
    "\n",
    "    # Search for active LSL stream\n",
    "    print('Looking for an EEG stream...')\n",
    "    streams = resolve_byprop('type', 'EEG', timeout=2)\n",
    "    if len(streams) == 0:\n",
    "        raise RuntimeError('Can\\'t find EEG stream.')\n",
    "\n",
    "    # Set active EEG stream to inlet and apply time correction\n",
    "    print(\"Im ready\")\n",
    "    print(\"IM ready\")\n",
    "    print(\"IM READY\")\n",
    "    inlet = StreamInlet(streams[0], max_chunklen=12)\n",
    "    eeg_time_correction = inlet.time_correction()\n",
    "\n",
    "    # Get the stream info, description, sampling frequency, number of channels\n",
    "    info = inlet.info()\n",
    "    description = info.desc()\n",
    "    fs = int(info.nominal_srate())\n",
    "    n_channels = info.channel_count()\n",
    "\n",
    "    # Get names of all channels\n",
    "    ch = description.child('channels').first_child()\n",
    "    ch_names = [ch.child_value('label')]\n",
    "    for i in range(1, n_channels):\n",
    "        ch = ch.next_sibling()\n",
    "        ch_names.append(ch.child_value('label'))\n",
    "\n",
    "    \"\"\" 2. SET EXPERIMENTAL PARAMETERS \"\"\"\n",
    "\n",
    "    # Length of the EEG data buffer (in seconds)\n",
    "    # This buffer will hold last n seconds of data and be used for calculations\n",
    "    buffer_length = 15\n",
    "\n",
    "    # Length of the epochs used to compute the FFT (in seconds)\n",
    "    epoch_length = 1\n",
    "\n",
    "    # Amount of overlap between two consecutive epochs (in seconds)\n",
    "    overlap_length = 0.8\n",
    "\n",
    "    # Amount to 'shift' the start of each next consecutive epoch\n",
    "    shift_length = epoch_length - overlap_length\n",
    "\n",
    "    # Index of the channel (electrode) to be used\n",
    "    # 0 = left ear, 1 = left forehead, 2 = right forehead, 3 = right ear\n",
    "    index_channel = [0, 1, 2, 3]\n",
    "    # Name of our channel for plotting purposes\n",
    "    ch_names = [ch_names[i] for i in index_channel]\n",
    "    n_channels = len(index_channel)\n",
    "\n",
    "    # Get names of features\n",
    "    # ex. ['delta - CH1', 'pwr-theta - CH1', 'pwr-alpha - CH1',...]\n",
    "    feature_names = BCIw.get_feature_names(ch_names)\n",
    "\n",
    "    # Number of seconds to collect training data for (one class)\n",
    "    training_length = 30\n",
    "\n",
    "    \"\"\" 3. RECORD TRAINING DATA \"\"\"\n",
    "\n",
    "    # Record data for mental activity 0\n",
    "    #winsound.Beep(400,1000)\n",
    "    eeg_data0, timestamps0 = inlet.pull_chunk(\n",
    "            timeout=training_length+1, max_samples=fs * training_length)\n",
    "    eeg_data0 = np.array(eeg_data0)[:, index_channel]\n",
    "\n",
    "    print('\\n Stimuli \\n')\n",
    "\n",
    "    # Record data for mental activity 1\n",
    "    winsound.Beep(400,1000) # Beep sound\n",
    "    eeg_data1, timestamps1 = inlet.pull_chunk(\n",
    "            timeout=training_length+1, max_samples=fs * training_length)\n",
    "    eeg_data1 = np.array(eeg_data1)[:, index_channel]\n",
    "\n",
    "    # Divide data into epochs\n",
    "    eeg_epochs0 = BCIw.epoch(eeg_data0, epoch_length * fs,\n",
    "                             overlap_length * fs)\n",
    "    eeg_epochs1 = BCIw.epoch(eeg_data1, epoch_length * fs,\n",
    "                             overlap_length * fs)\n",
    "\n",
    "    \"\"\" 4. COMPUTE FEATURES AND TRAIN CLASSIFIER \"\"\"\n",
    "\n",
    "    feat_matrix0 = BCIw.compute_feature_matrix(eeg_epochs0, fs)\n",
    "    feat_matrix1 = BCIw.compute_feature_matrix(eeg_epochs1, fs)\n",
    "\n",
    "    [classifier, mu_ft, std_ft] = BCIw.train_classifier(\n",
    "            feat_matrix0, feat_matrix1, 'SVM')\n",
    "\n",
    "    winsound.Beep(500,1000)\n",
    "\n",
    "    \"\"\" 5. USE THE CLASSIFIER IN REAL-TIME\"\"\"\n",
    "\n",
    "    # Initialize the buffers for storing raw EEG and decisions\n",
    "    eeg_buffer = np.zeros((int(fs * buffer_length), n_channels))\n",
    "    filter_state = None  # for use with the notch filter\n",
    "    decision_buffer = np.zeros((30, 1))\n",
    "\n",
    "    plotter_decision = BCIw.DataPlotter(30, ['Decision'])\n",
    "\n",
    "    # The try/except structure allows to quit the while loop by aborting the\n",
    "    # script with <Ctrl-C>\n",
    "    print('Press Ctrl-C in the console to break the while loop.')\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "\n",
    "            \"\"\" 3.1 ACQUIRE DATA \"\"\"\n",
    "            # Obtain EEG data from the LSL stream\n",
    "            eeg_data, timestamp = inlet.pull_chunk(\n",
    "                    timeout=1, max_samples=int(shift_length * fs))\n",
    "\n",
    "            # Only keep the channel we're interested in\n",
    "            ch_data = np.array(eeg_data)[:, index_channel]\n",
    "\n",
    "            # Update EEG buffer\n",
    "            eeg_buffer, filter_state = BCIw.update_buffer(\n",
    "                    eeg_buffer, ch_data, notch=True,\n",
    "                    filter_state=filter_state)\n",
    "\n",
    "            \"\"\" 3.2 COMPUTE FEATURES AND CLASSIFY \"\"\"\n",
    "            # Get newest samples from the buffer\n",
    "            data_epoch = BCIw.get_last_data(eeg_buffer,\n",
    "                                            epoch_length * fs)\n",
    "\n",
    "            # Compute features\n",
    "            feat_vector = BCIw.compute_feature_vector(data_epoch, fs)\n",
    "            y_hat = BCIw.test_classifier(classifier,\n",
    "                                         feat_vector.reshape(1, -1), mu_ft,\n",
    "                                         std_ft)\n",
    "            print(y_hat)\n",
    "\n",
    "            decision_buffer, _ = BCIw.update_buffer(decision_buffer,\n",
    "                                                    np.reshape(y_hat, (-1, 1)))\n",
    "\n",
    "            \"\"\" 3.3 VISUALIZE THE DECISIONS \"\"\"\n",
    "            plotter_decision.update_plot(decision_buffer)\n",
    "            plt.pause(0.00001)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        print('Closed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "            \"\"\" 3.1 ACQUIRE DATA \"\"\"\n",
    "            # Obtain EEG data from the LSL stream\n",
    "            eeg_data, timestamp = inlet.pull_chunk(\n",
    "                    timeout=1, max_samples=int(shift_length * fs))\n",
    "\n",
    "            # Only keep the channel we're interested in\n",
    "            ch_data = np.array(eeg_data)[:, index_channel]\n",
    "\n",
    "            # Update EEG buffer\n",
    "            eeg_buffer, filter_state = BCIw.update_buffer(\n",
    "                    eeg_buffer, ch_data, notch=True,\n",
    "                    filter_state=filter_state)\n",
    "\n",
    "            \"\"\" 3.2 COMPUTE FEATURES AND CLASSIFY \"\"\"\n",
    "            # Get newest samples from the buffer\n",
    "            data_epoch = BCIw.get_last_data(eeg_buffer,\n",
    "                                            epoch_length * fs)\n",
    "\n",
    "            # Compute features\n",
    "            feat_vector = BCIw.compute_feature_vector(data_epoch, fs)\n",
    "            y_hat = BCIw.test_classifier(classifier,\n",
    "                                         feat_vector.reshape(1, -1), mu_ft,\n",
    "                                         std_ft)\n",
    "            print(y_hat)\n",
    "\n",
    "            decision_buffer, _ = BCIw.update_buffer(decision_buffer,\n",
    "                                                    np.reshape(y_hat, (-1, 1)))\n",
    "\n",
    "            \"\"\" 3.3 VISUALIZE THE DECISIONS \"\"\"\n",
    "            plotter_decision.update_plot(decision_buffer)\n",
    "            plt.pause(0.00001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
